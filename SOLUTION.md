# Our approach

We tackle this challenge based on the assumption that high-quality data makes the model perform better than low-quality data. So, we firstly try to estimate the data quality for each data point. Then, we select top 200 high-quality data for fine-tuning the model.

## Dataset

We use the following datasets for fine-tuning:

* [LIMA](https://huggingface.co/datasets/GAIR/lima)
* [DOLLY](https://huggingface.co/datasets/databricks/databricks-dolly-15k)
* [GUANACO](https://huggingface.co/datasets/timdettmers/openassistant-guanaco)
* [PLATYPUS](https://huggingface.co/datasets/garage-bAInd/Open-Platypus) (exclude OpenAI-LLM generated data)

**NOTE**: According to table 1 in [Platypus paper](https://arxiv.org/abs/2308.07317), there are 2 datasets (*leetcode-solutions-python-testgen-gpt4*, *airoboros-gpt4-1.4.1*) that generated by GPT-4. We exclude these 2 datasets from our training data.

## Data Selection

Inspired by [InstructMining](https://arxiv.org/abs/2307.06290) paper, we use this method to estimate the quality (estimated loss) of each data point. 

$$
\log(L) \propto 1.0694 - 0.1498\text{Rew} + 8.257 * 10^{-5}\text{Len} - 0.9350\text{Knn}_6 + \epsilon
$$

Where $\text{Rew}$ is the reward score, $\text{Len}$ is the length of the response, and $\text{Knn}_6$ is the distance to $i^{th}$-nearest neighbor in the embedding space. 

Once we have the estimated loss of each data point, we select the top 200 data points from each dataset. Then, we merge all selected data points into one dataset which we already uploaded to HuggingFace Hub. You can download the dataset from [here](https://huggingface.co/datasets/lingjoor/lingjoor-dataset).

```zsh
# HOW-TO-RUN will update later if organizers ask us to do so.
```

## Training

We use [Qwen 14B](https://huggingface.co/Qwen/Qwen-14B) as pre-trained model. Then, fine-tune it on our selected data. To fine-tune the model, please follow the instructions below:

```zsh
# Download the dataset
$ git lfs install # if you haven't installed it yet
$ git clone https://huggingface.co/datasets/lingjoor/lingjoor-dataset  

# Install trainer
$ git clone https://github.com/kunato/axolotl
$ cd axolotl
$ pip install packaging
$ pip install -e '.[flash-attn,deepspeed]'
$ pip install -U git+https://github.com/huggingface/peft.git
$ pip install transformers_stream_generator

# Train the model
$ sh ../scripts/train.sh -t $YOUR_HUGGINGFACE_TOKEN
```

The script will automatically upload the model to HuggingFace Hub. 
